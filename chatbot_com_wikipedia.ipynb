{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMD0BAFMpQztDV9IGeMeHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spdr-lily/Data-Science-Profile/blob/main/chatbot_com_wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2QKGPG6cg0c",
        "outputId": "1565c374-899d-452a-be00-51c3f189bfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=998433b47e7c4c9aa292294a4906d0300c8402919f9802093d1a9ed433a73f24\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI_WacRhaRW6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "import random\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar recursos do NLTK\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26i3tFwKcsyc",
        "outputId": "a73e89d1-24a7-4e4f-a758-f1327aa6c4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar Wikipedia em português\n",
        "wikipedia.set_lang(\"pt\")"
      ],
      "metadata": {
        "id": "E5t3i3DUcoxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar modelo de análise de sentimentos baseado em Transformers\n",
        "sentiment_pipeline = pipeline(model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXO9Vs9NcvkD",
        "outputId": "5ebf0687-ff00-48a0-e031-7793da4452bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(user_input): # nalisa o sentimento da mensagem usando TextBlob, NLTK VADER e um modelo baseado em Transformers.\n",
        "    textblob_polarity = TextBlob(user_input).sentiment.polarity\n",
        "    vader_polarity = sia.polarity_scores(user_input)['compound']\n",
        "    transformer_sentiment = sentiment_pipeline(user_input)[0]['score']\n",
        "\n",
        "    # Ajustando o sinal da pontuação do Transformer para manter a consistência\n",
        "    if sentiment_pipeline(user_input)[0]['label'] == 'negative':\n",
        "        transformer_sentiment = -transformer_sentiment\n",
        "\n",
        "    # Média das análises\n",
        "    final_polarity = (textblob_polarity + vader_polarity + transformer_sentiment) / 3\n",
        "    return final_polarity"
      ],
      "metadata": {
        "id": "7iu0ye1Rc5CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input): # gerar uma resposta baseada na análise de sentimento com mais variações e limiares ajustados.\n",
        "    sentiment = analyze_sentiment(user_input)\n",
        "\n",
        "    positive_responses = [\n",
        "        \"Isso é ótimo! Como posso ajudar ainda mais?\",\n",
        "        \"Fico feliz em saber disso! O que mais posso fazer por você?\",\n",
        "        \"Que notícia maravilhosa! Conte-me mais.\"\n",
        "    ]\n",
        "\n",
        "    neutral_responses = [\n",
        "        \"Entendi. Você pode me contar mais?\",\n",
        "        \"Interessante! Quer compartilhar mais detalhes?\",\n",
        "        \"Ok! Como posso ajudar?\"\n",
        "    ]\n",
        "\n",
        "    slightly_negative_responses = [\n",
        "        \"Parece que algo está te incomodando. Quer me contar mais?\",\n",
        "        \"Sinto que há algo errado. Como posso ajudar?\",\n",
        "        \"Se precisar desabafar, estou aqui para ouvir.\"\n",
        "    ]\n",
        "\n",
        "    negative_responses = [\n",
        "        \"Sinto muito por isso. Como posso ajudar a melhorar seu dia?\",\n",
        "        \"Parece que você não está tendo um bom dia. Quer conversar mais sobre isso?\",\n",
        "        \"Lamento ouvir isso. Estou aqui para ajudar, se precisar.\"\n",
        "    ]\n",
        "\n",
        "    if sentiment > 0.5:\n",
        "        return random.choice(positive_responses)\n",
        "    elif sentiment > 0.1:\n",
        "        return random.choice(neutral_responses)\n",
        "    elif sentiment > -0.3:\n",
        "        return random.choice(slightly_negative_responses)\n",
        "    else:\n",
        "        return random.choice(negative_responses)"
      ],
      "metadata": {
        "id": "EQip_lJ6c7Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikipedia_info(query):\n",
        "    \"\"\"Busca informações da Wikipedia sobre o tema perguntado.\"\"\"\n",
        "    try:\n",
        "        resultados = wikipedia.search(query)\n",
        "        if not resultados:\n",
        "            return \"Desculpe, não encontrei informações sobre esse assunto.\"\n",
        "        melhor_resultado = resultados[0]\n",
        "        texto = wikipedia.page(melhor_resultado)\n",
        "        return f\"{texto.summary}\\n\\nFonte: {melhor_resultado} - Wikipedia\"\n",
        "    except Exception as e:\n",
        "        return f\"Houve um erro ao buscar informações: {str(e)}\""
      ],
      "metadata": {
        "id": "ghiLlUHDc9rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Chatbot: Olá! Como posso te ajudar hoje?\")\n",
        "    while True:\n",
        "        user_input = input(\"Você: \")\n",
        "        if user_input.lower() in [\"sair\", \"exit\", \"tchau\"]:\n",
        "            print(\"Chatbot: Até mais! Tenha um ótimo dia.\")\n",
        "            break\n",
        "\n",
        "        wiki_response = fetch_wikipedia_info(user_input)\n",
        "        print(\"Chatbot:\", wiki_response)\n",
        "\n",
        "        feedback = input(\"O que achou da resposta?\\n\")\n",
        "        sentimento = sentiment_pipeline(feedback)\n",
        "\n",
        "        if sentimento[0]['label'] == \"positive\" and sentimento[0]['score'] > 0.6:\n",
        "            print(\"Chatbot: Fico feliz que tenha gostado da resposta!\")\n",
        "        elif sentimento[0]['label'] == \"negative\" and sentimento[0]['score'] > 0.6:\n",
        "            print(\"Chatbot: Que pena que não gostou da resposta. Gostaria de tentar uma outra pergunta?\")\n",
        "        else:\n",
        "            print(\"Chatbot: Obrigado pelo feedback, volte sempre.\")"
      ],
      "metadata": {
        "id": "Q7fjy10tc_f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvBAXK2mdBZv",
        "outputId": "23b1a5d2-e300-437f-f9dc-847381102117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Olá! Como posso te ajudar hoje?\n",
            "Você: O que é PLN?\n",
            "Chatbot: Processamento de língua natural (PLN) é uma subárea da ciência da computação, inteligência artificial e da linguística que estuda os problemas da geração e compreensão automática de línguas humanas naturais. Sistemas de geração de língua natural convertem informação de bancos de dados de computadores em linguagem compreensível ao ser humano e sistemas de compreensão de língua natural convertem ocorrências de linguagem humana em representações mais formais, mais facilmente manipuláveis por programas de computador. Alguns desafios do PLN são compreensão de língua natural, fazer com que computadores extraiam sentido de linguagem humana ou natural e geração de língua natural.\n",
            "\n",
            "Fonte: Processamento de linguagem natural - Wikipedia\n",
            "O que achou da resposta?\n",
            "Eu gostei muito!\n",
            "Chatbot: Fico feliz que tenha gostado da resposta!\n",
            "Você: tchau\n",
            "Chatbot: Até mais! Tenha um ótimo dia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzYKyUZGdCmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}